# üîÆ Integraci√≥n de Ollama - Tarotista IA

## Descripci√≥n

Se ha integrado el servicio **Ollama** con el modelo **llama3** al proyecto TuTarot. Ahora, despu√©s de que se realiza una lectura de tarot de 3 cartas, aparecer√° una secci√≥n **"Tarotista IA"** que proporcionar√° una interpretaci√≥n personalizada basada en:

- Datos del usuario (nombre, edad, estado sentimental, pareja)
- Contexto personal (pasado, presente, futuro)
- Las 3 cartas asignadas (pasado, presente, futuro)

## Requisitos Previos

1. **Tener Ollama instalado** en tu sistema
   - Descargar desde: https://ollama.ai/

2. **Tener el modelo `llama3` descargado**
   - Ejecutar en terminal:

     ```bash
     ollama pull llama3
     ```

3. **Ollama debe estar ejecut√°ndose** en `http://localhost:11434`
   - Por defecto, Ollama se ejecuta en este puerto
   - Si necesitas cambiar el puerto, edita la variable `OLLAMA_API` en `js/ollamaService.js`

## C√≥mo Funciona

### 1. **Flujo de Ejecuci√≥n**

```
Usuario completa formulario
        ‚Üì
Se seleccionan 3 cartas al azar
        ‚Üì
Se muestran las cartas en el modal
        ‚Üì
Sistema env√≠a datos a Ollama
        ‚Üì
Ollama genera interpretaci√≥n con rol "Tarotista"
        ‚Üì
Texto se muestra letra por letra en "Tarotista IA"
```

### 2. **El Prompt del Tarotista**

El sistema env√≠a a Ollama un prompt que incluye:

- **Rol**: "Tarotista experto y emp√°tico"
- **Instrucci√≥n**: 
  > "Necesito que como tarotista le expliques bien al usuario acerca de cada carta que se le asign√≥ en base a Pasado, Presente y Futuro y los datos aportados por el usuario. Por √∫ltimo resumile el resultado final con consejos para afrontarlo."

- **Contexto del usuario**: Todos los datos del formulario
- **Cartas**: Nombres y descripciones de las 3 cartas seleccionadas

### 3. **Visualizaci√≥n Letra por Letra**

La respuesta de Ollama aparece en la secci√≥n "Tarotista IA" **letra por letra** usando `setInterval` con un delay de 50ms por car√°cter, creando un efecto de escritura en tiempo real.

## Archivos Modificados/Creados

### ‚ú® Nuevos Archivos

- **`js/ollamaService.js`** - Servicio que maneja la comunicaci√≥n con Ollama

### üìù Archivos Modificados

1. **`js/main-tarot-love-three-cards.js`**
   - Agregadas variables globales para almacenar datos del formulario y cartas
   - Agregada funci√≥n `generateTarotistInterpretation()` para llamar a Ollama
   - Agregada funci√≥n `displayTextLetterByLetter()` para mostrar el texto con efecto de escritura
   - Modificada `renderModalContent()` para incluir la nueva secci√≥n "Tarotista IA"

2. **`css/styles.css`**
   - Agregados estilos para `.tarotista-ia-section`
   - Agregados estilos para `#tarotista-response`
   - Agregada animaci√≥n `fadeIn` para la secci√≥n

3. **`pages/tarot-love-three-cards.html`**
   - (Sin cambios directos, los cambios se hacen via JavaScript)

## Configuraci√≥n

### Cambiar el Modelo (opcional)

Si deseas usar otro modelo de Ollama, edita `js/ollamaService.js`:

```javascript
const MODEL = 'llama3'; // Cambiar aqu√≠
```

Modelos disponibles:
- `llama3` (recomendado)
- `mistral`
- `neural-chat`
- `orca-mini`
- Otros disponibles en Ollama

### Cambiar Velocidad de Escritura (opcional)

En `js/main-tarot-love-three-cards.js`, en la funci√≥n `displayTextLetterByLetter()`:

```javascript
const speed = 50; // Milisegundos entre cada car√°cter (ajusta este valor)
```

### Cambiar la URL de Ollama (si est√° en otro servidor)

En `js/ollamaService.js`:

```javascript
const OLLAMA_API = 'http://localhost:11434/api/generate'; // Cambiar URL aqu√≠
```

## Soluci√≥n de Problemas

### ‚ùå "No se pudo conectar con Ollama"

**Soluciones:**

1. Verifica que Ollama est√© ejecut√°ndose:
   ```bash
   ollama serve
   ```

2. Aseg√∫rate de que el modelo est√© descargado:
   ```bash
   ollama list
   ```

3. Si no est√°, descargalo:
   ```bash
   ollama pull llama3
   ```

4. Verifica que puedas acceder a Ollama:
   - Abre en navegador: `http://localhost:11434`
   - Deber√≠as ver: `Ollama is running`

### ‚è≥ La respuesta tarda mucho

- Esto es normal, especialmente la primera vez
- El modelo `llama3` puede tardar 30-60 segundos en generar la respuesta
- Depende del hardware de tu m√°quina

### üìù El texto no aparece letra por letra

- Abre las DevTools (F12) y revisa la consola
- Verifica que no haya errores de JavaScript
- Aseg√∫rate de que `ollamaService.js` se est√° cargando correctamente

## Caracter√≠sticas T√©cnicas

- ‚úÖ Integraci√≥n as√≠ncrona con Ollama
- ‚úÖ Manejo de errores robusto
- ‚úÖ Estilos responsivos
- ‚úÖ Animaci√≥n suave de aparici√≥n
- ‚úÖ Scroll autom√°tico al generar texto
- ‚úÖ Almacenamiento temporal de datos del usuario y cartas

## Pr√≥ximas Mejoras Posibles

- [ ] Agregar opci√≥n de guardar respuesta
- [ ] Agregar opci√≥n de regenerar interpretaci√≥n
- [ ] Estad√≠sticas de interpretaciones
- [ ] Soporte para m√∫ltiples idiomas
- [ ] Cach√© de interpretaciones
- [ ] Opci√≥n para cambiar velocidad de escritura
- [ ] Tema oscuro/claro para la secci√≥n IA

---

**Versi√≥n**: 1.0  
**√öltimo actualizado**: 28 de diciembre de 2025
